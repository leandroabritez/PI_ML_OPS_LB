{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'polarity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mg:\\Mi unidad\\Henry\\PT06\\PI_01\\PI_ML_OPS_LB\\pruebas.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Ejemplo de uso\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m sample_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mEste es un ejemplo de análisis de sentimiento con spaCy. ¡Me encanta!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m sentiment_label \u001b[39m=\u001b[39m analyze_sentiment(sample_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W0sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAnálisis de sentimiento: \u001b[39m\u001b[39m{\u001b[39;00msentiment_label\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mg:\\Mi unidad\\Henry\\PT06\\PI_01\\PI_ML_OPS_LB\\pruebas.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Obtener el puntaje de polaridad del análisis de sentimiento\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m sentiment_score \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49msentiment\u001b[39m.\u001b[39;49mpolarity\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Definir umbrales para clasificar el sentimiento\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m sentiment_score \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.2\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'polarity'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "# Cargar el modelo de lenguaje de spaCy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Procesar el texto con spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Obtener el puntaje de polaridad del análisis de sentimiento\n",
    "    sentiment_score = doc.sentiment.polarity\n",
    "    \n",
    "    # Definir umbrales para clasificar el sentimiento\n",
    "    if sentiment_score <= -0.2:\n",
    "        return 0  # Sentimiento negativo\n",
    "    elif sentiment_score >= 0.2:\n",
    "        return 2  # Sentimiento positivo\n",
    "    else:\n",
    "        return 1  # Sentimiento neutral\n",
    "\n",
    "# Ejemplo de uso\n",
    "sample_text = \"Este es un ejemplo de análisis de sentimiento con spaCy. ¡Me encanta!\"\n",
    "sentiment_label = analyze_sentiment(sample_text)\n",
    "print(f\"Análisis de sentimiento: {sentiment_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\leabr/nltk_data'\n",
      "    - 'c:\\\\Users\\\\leabr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\leabr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\leabr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\leabr\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "    - ''\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     36\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\tokenizers.py:57\u001b[0m, in \u001b[0;36mSentenceTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Return a list of sentences.'''\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[39mreturn\u001b[39;00m nltk\u001b[39m.\u001b[39;49mtokenize\u001b[39m.\u001b[39;49msent_tokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\leabr/nltk_data'\n    - 'c:\\\\Users\\\\leabr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\leabr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\leabr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\leabr\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mg:\\Mi unidad\\Henry\\PT06\\PI_01\\PI_ML_OPS_LB\\pruebas.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m doc\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39mblob\u001b[39m.\u001b[39msubjectivity                        \u001b[39m# Subjectivity: 0.9\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m doc\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39mblob\u001b[39m.\u001b[39msentiment_assessments\u001b[39m.\u001b[39massessments   \u001b[39m# Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m doc\u001b[39m.\u001b[39;49m_\u001b[39m.\u001b[39;49mblob\u001b[39m.\u001b[39;49mngrams()                            \u001b[39m# [WordList(['I', 'had', 'a']), WordList(['had', 'a', 'really']), WordList(['a', 'really', 'horrible']), WordList(['really', 'horrible', 'day']), WordList(['horrible', 'day', 'It']), WordList(['day', 'It', 'was']), WordList(['It', 'was', 'the']), WordList(['was', 'the', 'worst']), WordList(['the', 'worst', 'day']), WordList(['worst', 'day', 'ever']), WordList(['day', 'ever', 'But']), WordList(['ever', 'But', 'every']), WordList(['But', 'every', 'now']), WordList(['every', 'now', 'and']), WordList(['now', 'and', 'then']), WordList(['and', 'then', 'I']), WordList(['then', 'I', 'have']), WordList(['I', 'have', 'a']), WordList(['have', 'a', 'really']), WordList(['a', 'really', 'good']), WordList(['really', 'good', 'day']), WordList(['good', 'day', 'that']), WordList(['day', 'that', 'makes']), WordList(['that', 'makes', 'me']), WordList(['makes', 'me', 'happy'])]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\blob.py:520\u001b[0m, in \u001b[0;36mBaseBlob.ngrams\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    518\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m    519\u001b[0m grams \u001b[39m=\u001b[39m [WordList(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords[i:i \u001b[39m+\u001b[39m n])\n\u001b[1;32m--> 520\u001b[0m                     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwords) \u001b[39m-\u001b[39m n \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)]\n\u001b[0;32m    521\u001b[0m \u001b[39mreturn\u001b[39;00m grams\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\decorators.py:24\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m---> 24\u001b[0m value \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(obj)\n\u001b[0;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\blob.py:649\u001b[0m, in \u001b[0;36mTextBlob.words\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[39m@cached_property\u001b[39m\n\u001b[0;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    643\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a list of word tokens. This excludes punctuation characters.\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[39m    If you want to include punctuation characters, access the ``tokens``\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[39m    property.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \n\u001b[0;32m    647\u001b[0m \u001b[39m    :returns: A :class:`WordList <WordList>` of word tokens.\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 649\u001b[0m     \u001b[39mreturn\u001b[39;00m WordList(word_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw, include_punc\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\tokenizers.py:73\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, include_punc, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, include_punc\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     65\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convenience function for tokenizing text into words.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[39m    NOTE: NLTK's word tokenizer expects sentences as input, so the text will be\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39m    tokenized to sentences before being tokenized to words.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     words \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[0;32m     71\u001b[0m         _word_tokenizer\u001b[39m.\u001b[39mitokenize(sentence, include_punc\u001b[39m=\u001b[39minclude_punc,\n\u001b[0;32m     72\u001b[0m                                 \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 73\u001b[0m         \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sent_tokenize(text))\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m words\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\base.py:64\u001b[0m, in \u001b[0;36mBaseTokenizer.itokenize\u001b[1;34m(self, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mitokenize\u001b[39m(\u001b[39mself\u001b[39m, text, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a generator that generates tokens \"on-demand\".\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39m    :rtype: generator\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     \u001b[39mreturn\u001b[39;00m (t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(text, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\textblob\\decorators.py:38\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     37\u001b[0m     \u001b[39mprint\u001b[39m(err)\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mraise\u001b[39;00m MissingCorpusError()\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "# the following installations are required\n",
    "# python -m textblob.download_corpora\n",
    "# python -m spacy download en_core_web_sm\n",
    "import nltk\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "doc = nlp(text)\n",
    "doc._.blob.polarity                            # Polarity: -0.125\n",
    "doc._.blob.subjectivity                        # Subjectivity: 0.9\n",
    "doc._.blob.sentiment_assessments.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n",
    "doc._.blob.ngrams()                            # [WordList(['I', 'had', 'a']), WordList(['had', 'a', 'really']), WordList(['a', 'really', 'horrible']), WordList(['really', 'horrible', 'day']), WordList(['horrible', 'day', 'It']), WordList(['day', 'It', 'was']), WordList(['It', 'was', 'the']), WordList(['was', 'the', 'worst']), WordList(['the', 'worst', 'day']), WordList(['worst', 'day', 'ever']), WordList(['day', 'ever', 'But']), WordList(['ever', 'But', 'every']), WordList(['But', 'every', 'now']), WordList(['every', 'now', 'and']), WordList(['now', 'and', 'then']), WordList(['and', 'then', 'I']), WordList(['then', 'I', 'have']), WordList(['I', 'have', 'a']), WordList(['have', 'a', 'really']), WordList(['a', 'really', 'good']), WordList(['really', 'good', 'day']), WordList(['good', 'day', 'that']), WordList(['day', 'that', 'makes']), WordList(['that', 'makes', 'me']), WordList(['makes', 'me', 'happy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de sentimiento: (1, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Crear un objeto TextBlob con el texto\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Obtener la polaridad del sentimiento (-1 a 1, donde -1 es negativo, 0 es neutral y 1 es positivo)\n",
    "    sentiment_score = blob.sentiment.polarity\n",
    "    \n",
    "    # Definir umbrales para clasificar el sentimiento\n",
    "    if sentiment_score <= -0.2:\n",
    "        return 0, sentiment_score  # Sentimiento negativo\n",
    "    elif sentiment_score >= 0.04:\n",
    "        return 2, sentiment_score  # Sentimiento positivo\n",
    "    else:\n",
    "        return 1, sentiment_score  # Sentimiento neutral\n",
    "\n",
    "# Ejemplo de uso\n",
    "sample_text = \"Bad,people just spam 1 op character 1/10 \"\n",
    "sentiment_label = analyze_sentiment(sample_text)\n",
    "print(f\"Análisis de sentimiento: {sentiment_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\Mi unidad\\Henry\\PT06\\PI_01\\PI_ML_OPS_LB\\pruebas.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m blob\u001b[39m.\u001b[39msentiment\u001b[39m.\u001b[39mpolarity\n",
      "\u001b[1;31mNameError\u001b[0m: name 'blob' is not defined"
     ]
    }
   ],
   "source": [
    "blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'float' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\Mi unidad\\Henry\\PT06\\PI_01\\PI_ML_OPS_LB\\pruebas.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Ejemplo de uso\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m genero_consultado \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mAction\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m año_resultado \u001b[39m=\u001b[39m PlayTimeGenre(genero_consultado)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPara el género \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mgenero_consultado\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, el año con más horas jugadas es \u001b[39m\u001b[39m{\u001b[39;00maño_resultado\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mg:\\Mi unidad\\Henry\\PT06\\PI_01\\PI_ML_OPS_LB\\pruebas.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mPlayTimeGenre\u001b[39m(genero):\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Filtrar los juegos por el género proporcionado\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     juegos_por_genero \u001b[39m=\u001b[39m steam_games[steam_games[\u001b[39m'\u001b[39;49m\u001b[39mgenres\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: genero \u001b[39min\u001b[39;49;00m x)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Inicializar un diccionario para almacenar las horas jugadas por año\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     horas_por_año \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4758\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   4759\u001b[0m         func,\n\u001b[0;32m   4760\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[0;32m   4761\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[0;32m   4762\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   4763\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m-> 4764\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[0;32m   1290\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[0;32m   1291\u001b[0m )\n\u001b[0;32m   1293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[1;32mc:\\Users\\leabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mg:\\Mi unidad\\Henry\\PT06\\PI_01\\PI_ML_OPS_LB\\pruebas.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mPlayTimeGenre\u001b[39m(genero):\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Filtrar los juegos por el género proporcionado\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     juegos_por_genero \u001b[39m=\u001b[39m steam_games[steam_games[\u001b[39m'\u001b[39m\u001b[39mgenres\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: genero \u001b[39min\u001b[39;49;00m x)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Inicializar un diccionario para almacenar las horas jugadas por año\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Mi%20unidad/Henry/PT06/PI_01/PI_ML_OPS_LB/pruebas.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     horas_por_año \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'float' is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Cargar los archivos CSV\n",
    "steam_games = pd.read_csv('steam_games.csv')\n",
    "user_items = pd.read_csv('user_items.csv', converters={'items': literal_eval})\n",
    "\n",
    "# Función para obtener el año con más horas jugadas para un género dado\n",
    "def PlayTimeGenre(genero):\n",
    "    # Filtrar los juegos por el género proporcionado\n",
    "    juegos_por_genero = steam_games[steam_games['genres'].apply(lambda x: genero in x)]\n",
    "\n",
    "    # Inicializar un diccionario para almacenar las horas jugadas por año\n",
    "    horas_por_año = {}\n",
    "\n",
    "    # Iterar sobre los juegos del género\n",
    "    for index, juego in juegos_por_genero.iterrows():\n",
    "        # Obtener el ID del juego\n",
    "        juego_id = juego['id']\n",
    "\n",
    "        # Filtrar las entradas correspondientes en user_items\n",
    "        entradas_juego = user_items[user_items['items'].apply(lambda x: any(item.get('item_id') == juego_id for item in x))]\n",
    "\n",
    "        # Iterar sobre las entradas del juego\n",
    "        for _, entrada in entradas_juego.iterrows():\n",
    "            # Obtener el año del juego\n",
    "            año_juego = pd.to_datetime(juego['release_date']).year\n",
    "\n",
    "            # Obtener las horas jugadas\n",
    "            horas_jugadas = sum(item.get('playtime_forever', 0) for item in entrada['items'] if item.get('item_id') == juego_id)\n",
    "\n",
    "            # Agregar las horas jugadas al diccionario por año\n",
    "            if año_juego in horas_por_año:\n",
    "                horas_por_año[año_juego] += horas_jugadas\n",
    "            else:\n",
    "                horas_por_año[año_juego] = horas_jugadas\n",
    "\n",
    "    # Encontrar el año con más horas jugadas\n",
    "    año_mas_horas = max(horas_por_año, key=horas_por_año.get)\n",
    "\n",
    "    return año_mas_horas\n",
    "\n",
    "# Ejemplo de uso\n",
    "genero_consultado = 'Action'\n",
    "año_resultado = PlayTimeGenre(genero_consultado)\n",
    "print(f\"Para el género '{genero_consultado}', el año con más horas jugadas es {año_resultado}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "user_items = pd.read_csv('user_items.csv', converters={'items': literal_eval})\n",
    "\n",
    "# Inicializar un diccionario para almacenar la sumatoria de horas por item_id\n",
    "sumatoria_horas_por_item = {}\n",
    "\n",
    "# Iterar sobre las entradas del archivo user_items\n",
    "for _, entrada in user_items.iterrows():\n",
    "    # Iterar sobre los diccionarios en la columna 'items'\n",
    "    for item in entrada['items']:\n",
    "        # Obtener el item_id y las horas jugadas\n",
    "        item_id = item.get('item_id')\n",
    "        horas_jugadas = item.get('playtime_forever', 0)\n",
    "\n",
    "        # Agregar las horas jugadas al diccionario por item_id\n",
    "        if item_id in sumatoria_horas_por_item:\n",
    "            sumatoria_horas_por_item[item_id] += horas_jugadas\n",
    "        else:\n",
    "            sumatoria_horas_por_item[item_id] = horas_jugadas\n",
    "\n",
    "# Crear un DataFrame a partir del diccionario\n",
    "sumatoria_horas_df = pd.DataFrame(list(sumatoria_horas_por_item.items()), columns=['item_id', 'sumatoria_horas'])\n",
    "\n",
    "# Guardar el nuevo archivo CSV\n",
    "sumatoria_horas_df.to_csv('sumatoria_horas_por_item.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Año de lanzamiento con más horas jugadas para Género 'Strategy': 2013\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def PlayTimeGenre(genero: str):\n",
    "    # Cargar el DataFrame resultante del proceso anterior\n",
    "    result_grouped = pd.read_csv('PlayTimeGenre.csv')\n",
    "\n",
    "    # Filtrar por el género específico\n",
    "    genre_data = result_grouped[result_grouped['genres'] == genero]\n",
    "\n",
    "    if not genre_data.empty:\n",
    "        # Encontrar el año con más horas jugadas\n",
    "        max_year = int(genre_data.loc[genre_data['sumatoria_horas'].idxmax(), 'year'])\n",
    "        max_hours = genre_data['sumatoria_horas'].max()\n",
    "\n",
    "        return f\"Año de lanzamiento con más horas jugadas para Género '{genero}': {max_year}\"\n",
    "    else:\n",
    "        return f\"No hay información para el género '{genero}'.\"\n",
    "\n",
    "# Ejemplo de uso\n",
    "genero_buscar = 'Strategy'\n",
    "resultado = PlayTimeGenre(genero_buscar)\n",
    "print(resultado)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
